Parameters for human voice analysis
Frame shift
overlap
Window length:20~30ms
Hop size: 10ms
FFT points: 256 or 512
2 4 8 16 32 64 128 256 512 1024 2048 4096 8192


adobe audition

oceanaudio (스펙트로그램 그리기, fft 등)
-rectangular
-hamming
-blackman

feature extraction 방법: 크게 두가지
-발성모델: 어떻게 소리 내는가? LPCC
-청각모델: 어떻게 듣는가? 저주파, 고주파로 나눠서 하기도 하고.. 결국 MFCC를 많이 쓰게됨

framing: hop size, window size 등 조절, 원하는 부분 추출하기 위해 rectangular window를 곱함(원하는 부분 1, 나머지는 0)
windowing:  rectangular window는 앞뒤에서 직각으로 끊어지기 때문에 부드럽게 해주는 방법.
왜곡이 적은 윈도우 함수를 찾다보니 여러가지 윈도우 함수 개발.. 예. Hamming, blackman, ...



DCT – discrete cosine transform

CMS - 발화의 특성만 남기고 채널정보(예. 반사음, 노이즈, 남/여 차이...)는 없어지도록..

inverse fft하면 원래 신호 그대로 복원되나??

스펙트럼 -->스펙트로그램

Mel filterbank - 사람의 청각기관 모델링, 고주파 성분을 약하게..

play-wav.ipynb: 에러날 때 color지정 필요
librosa.display.waveshow(y = samples, sr = sampling_rate, color='black')

stft = librosa.stft(samples, n_fft=n_fft, hop_length=hop_length)
print("stft shape: ", stft.shape) #대략 n_fft의 반정도 숫자 나옴. 
print("sample number/hop_length: ", len(samples)/hop_length)

deep learning 모델에는 mel spectrogram이 들어간다..

specaug – warp, masking  등으로 데이터 일부 변형 → data augmentation 효과

log(W|X) = given X, probability of W, where X is mel spectrogram, W is word or speech token

g2p – grapheme to phoneme (발음 모델)
